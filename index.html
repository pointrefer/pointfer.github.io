<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PointRefer">
  <meta name="keywords" content="PointRefer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PointRefer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">PointRefer: Grasp the Inter-class Relations for 3D Visual Grounding with Referred Point Selection</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=CEEtEnoAAAAJ&hl=zh-CN">Hao Liu</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=jc58aTgAAAAJ&hl=zh-CN">Yanni Ma</a><sup>2</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=WQRNvdsAAAAJ&hl=zh-CN">Yulan Guo</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=ISNmBxwAAAAJ&hl=zh-CN">Ying He</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Nanyang Technological University</span> &nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Sun Yat-Sen University</span> &nbsp;&nbsp;&nbsp;<br>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PointRefer Video</title>
    <style>
        /* 视频居中和调整大小 */
        .video-container {
            display: flex; /* 使用 Flexbox 布局 */
            justify-content: center; /* 水平居中 */
            align-items: center; /* 垂直居中 */
            flex-direction: column; /* 垂直排列文本和视频 */
            height: 100vh; /* 视口高度 */
            background-color: #f9f9f9; /* 背景颜色（可选） */
        }

        video {
            width: 90%; /* 视频宽度占容器的 90% */
            max-width: 800px; /* 设置最大宽度 */
            border: 2px solid #ccc; /* 边框 */
            border-radius: 10px; /* 圆角 */
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); /* 阴影 */
        }

        h2.subtitle {
            margin-bottom: 20px; /* 增加标题与视频的间距 */
            text-align: center; /* 标题居中 */
            font-family: Arial, sans-serif; /* 字体（可选） */
            color: #333; /* 字体颜色 */
        }
    </style>
</head>
<body>
    <section class="video-container">
        <h2 class="subtitle has-text-justified">
            <b>TL;DR:</b> We propose a point-based single-stage method PointRefer, designed to grasp the inter-class object relations implied in the language description to distinguish the target object from similar objects of the same category.
        </h2>
        <video controls>
            <source src="file/demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </section>
</body>
</html>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D visual grounding aims to locate a target object within point clouds based on a free-form language description. Existing methods typically extract sentence-level features by coupling all words together and use a language-to-object classifier to predict the target category. Consequently, these methods focus more on object categories while neglecting auxiliary objects and inter-class spatial relations in the query text. Additionally, these methods struggle to localize the referential target when similar objects are present. 
            To address these issues, we propose a point-based single-stage method PointRefer, designed to grasp the inter-class object relations implied in the language description. PointRefer decouples textual entities in the input text and learns the spatial relations between entities (i.e., the main object and auxiliary objects) to accurately locate the referential target. Specifically, we first introduce a text parsing module that generates textual features for the Main object and Auxiliary object components in the input text. Then, we propose a language-aware sampling module, which selects a set of keypoints and auxiliary points based on the textual features of these parsed components. Finally, we develop a relation-aware transformer module that focuses on the target object's points. This module models the relationships between any two keypoints and auxiliary points, and then employs cross-modality attention layers to  refine keypoint selection in a coarse-to-fine manner. Extensive experiments show that our PointRefer achieves comparable or superior performance to existing methods on both the ScanRefer and Nr3D/Sr3D datasets.
          </p>
        </div>
        <img id="painting_icon" width="70%" src="file/model.PNG">
      </div>
    </div>
</section>

<section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Contribution</h2>
        <div class="content has-text-justified">
          <p>
            Our main contributions are
            </p><ol type="1">
              <span style="font-size: 95%;">We propose a point-based single-stage method, PointRefer, for 3D VG, which explicitly utilizes the inter-class relationship implied in natural language descriptions to locate the referential target. </span>
              <span style="font-size: 95%;">We propose a relation-aware transformer, which captures the complex object relations through distance encoding and progressively refines target-related keypoints by aligning language features using cross-modality attention.</span>
              <span style="font-size: 95%;">Extensive experiments on the ScanRefer and Nr3D/Sr3D datasets demonstrate the strong performance of our PointRefer, particularly in scenes where similar objects of the same category are present.</span>
            </ol>  
          <p></p>

        </div>
      </div>
    </div>      
  </div>
</section>

</body>
</html>
